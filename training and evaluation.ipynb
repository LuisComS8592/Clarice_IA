{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": [],
   "gpuType": "T4"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "language_info": {
   "name": "python"
  },
  "accelerator": "GPU"
 },
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TiCeuJ-Qmbp6"
   },
   "outputs": [],
   "source": [
    "!nvidia-smi  # Executa o utilit√°rio NVIDIA System Management Interface (nvidia-smi)\n",
    "             # Esse comando exibe informa√ß√µes sobre as GPUs NVIDIA dispon√≠veis no sistema,\n",
    "             # incluindo:\n",
    "             # - Modelo da GPU\n",
    "             # - Vers√£o do driver NVIDIA\n",
    "             # - Utiliza√ß√£o da GPU (mem√≥ria, processamento)\n",
    "             # - Processos que est√£o usando a GPU\n",
    "             #\n",
    "             # √â frequentemente usado em ambientes como Google Colab, Jupyter Notebooks ou\n",
    "             # servidores remotos para confirmar:\n",
    "             # - Se existe uma GPU dispon√≠vel\n",
    "             # - Se ela est√° corretamente configurada\n",
    "             # - Se h√° recursos suficientes (ex: mem√≥ria) antes de treinar modelos\n",
    "             #\n",
    "             # Caso n√£o haja GPU instalada ou reconhecida, geralmente o comando\n",
    "             # retornar√° erro ou mostrar√° \"No devices were found\"."
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "# Instala as bibliotecas necess√°rias para trabalhar com modelos de NLP da Hugging Face\n",
    "\n",
    "!pip install transformers   # Instala a biblioteca Transformers, que fornece:\n",
    "                            # - Modelos pr√©-treinados (BERT, GPT, T5, etc.)\n",
    "                            # - Pipelines prontos (tradu√ß√£o, resumo, classifica√ß√£o, etc.)\n",
    "                            # - Ferramentas para fine-tuning e infer√™ncia\n",
    "\n",
    "!pip install accelerate     # Instala a biblioteca Accelerate, usada para:\n",
    "                            # - Acelerar treinos em GPU/TPU\n",
    "                            # - Facilitar treinos distribu√≠dos (multi-GPU ou multi-n√≥)\n",
    "                            # - Otimizar performance sem mudar o c√≥digo do modelo"
   ],
   "metadata": {
    "id": "tGGclN3HWx7c"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "from google.colab import files\n",
    "import os\n",
    "\n",
    "# Nome esperado para o arquivo de corpus que ser√° utilizado no treinamento ou processamento NLP\n",
    "NOME_ARQUIVO_CORPUS = \"corpus.txt\"\n",
    "\n",
    "# Verifica se o arquivo j√° existe no diret√≥rio atual (evita upload duplicado)\n",
    "if not os.path.exists(NOME_ARQUIVO_CORPUS):\n",
    "\n",
    "  # Solicita ao usu√°rio o upload manual do arquivo\n",
    "  print(f\"Fazendo upload do arquivo '{NOME_ARQUIVO_CORPUS}'...\")\n",
    "  print(\"Por favor, selecione o seu arquivo de corpus.\")\n",
    "\n",
    "  # Abre o seletor de arquivos do Google Colab para upload\n",
    "  uploaded = files.upload()\n",
    "\n",
    "  # Verifica se o arquivo enviado tem exatamente o nome esperado\n",
    "  if NOME_ARQUIVO_CORPUS in uploaded:\n",
    "    print(f\"\\nArquivo '{NOME_ARQUIVO_CORPUS}' carregado com sucesso!\")\n",
    "  else:\n",
    "    # Se o nome for diferente, o script alerta o usu√°rio\n",
    "    print(f\"\\nERRO: O arquivo carregado n√£o se chama '{NOME_ARQUIVO_CORPUS}'.\")\n",
    "else:\n",
    "  # Caso o arquivo j√° exista, evita novo upload\n",
    "  print(f\"Arquivo '{NOME_ARQUIVO_CORPUS}' j√° existe no ambiente.\")\n"
   ],
   "metadata": {
    "id": "jaUi49wFXjjR"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "import torch\n",
    "from transformers import (\n",
    "    GPT2Tokenizer,\n",
    "    GPT2LMHeadModel,\n",
    "    TextDataset,\n",
    "    DataCollatorForLanguageModeling,\n",
    "    Trainer,\n",
    "    TrainingArguments\n",
    ")\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "# Modelo base pr√©-treinado em portugu√™s ‚Äî √≥timo ponto de partida para fine-tuning.\n",
    "# Este modelo j√° passou por treinamento com textos portugueses, o que acelera o aprendizado.\n",
    "MODELO_BASE = \"pierreguillou/gpt2-small-portuguese\"\n",
    "\n",
    "# Nome do arquivo de corpus que ser√° usado para treinar o modelo.\n",
    "# Ele deve estar no diret√≥rio de trabalho atual.\n",
    "ARQUIVO_CORPUS = \"corpus.txt\"\n",
    "\n",
    "# Diret√≥rio onde o modelo fine-tunado ser√° salvo ap√≥s o treinamento.\n",
    "PASTA_SAIDA = \"./modelo_gpt2\"\n",
    "\n",
    "\n",
    "print(f\"Carregando Tokenizador do modelo base: {MODELO_BASE}...\")\n",
    "# O tokenizador converte texto puro em tokens num√©ricos (IDs) entendidos pelo modelo.\n",
    "# Cada token representa uma palavra, subpalavra ou s√≠mbolo.\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(MODELO_BASE)\n",
    "\n",
    "print(f\"Carregando o Modelo base: {MODELO_BASE}...\")\n",
    "# GPT2LMHeadModel √© uma variante do GPT-2 voltada para tarefas de modelagem de linguagem (Language Modeling),\n",
    "# ou seja, prever o pr√≥ximo token em uma sequ√™ncia ‚Äî ideal para gera√ß√£o de texto.\n",
    "model = GPT2LMHeadModel.from_pretrained(MODELO_BASE)\n",
    "\n",
    "print(\"Tokenizador e Modelo carregados com sucesso!\")\n"
   ],
   "metadata": {
    "id": "ym7jrBL3YL2v"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# Vamos dividir o corpus manualmente para garantir que o modelo\n",
    "# seja avaliado em dados nunca vistos durante o treino (evita overfitting).\n",
    "print(\"‚úÇÔ∏è  Dividindo o corpus em Treino (90%) e Valida√ß√£o (10%)...\")\n",
    "\n",
    "# L√™ o arquivo completo do corpus como uma √∫nica string\n",
    "with open(ARQUIVO_CORPUS, \"r\", encoding=\"utf-8\") as f:\n",
    "    texto_completo = f.read()\n",
    "\n",
    "# Calcula o ponto de corte baseado em 90% do tamanho total do texto\n",
    "tamanho_corte = int(len(texto_completo) * 0.9)\n",
    "\n",
    "# Separa o texto em duas partes:\n",
    "# - Treinamento: 90%\n",
    "# - Valida√ß√£o: 10%\n",
    "texto_treino = texto_completo[:tamanho_corte]\n",
    "texto_validacao = texto_completo[tamanho_corte:]\n",
    "\n",
    "# Salva os splits como arquivos tempor√°rios\n",
    "with open(\"train_split.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(texto_treino)\n",
    "\n",
    "with open(\"val_split.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(texto_validacao)\n",
    "\n",
    "# Cria datasets tokenizados a partir dos arquivos, com blocos fixos\n",
    "# block_size define o tamanho de sequ√™ncia que o GPT-2 vai \"ver\" por vez\n",
    "train_dataset = TextDataset(\n",
    "    tokenizer=tokenizer,\n",
    "    file_path=\"train_split.txt\",\n",
    "    block_size=128\n",
    ")\n",
    "\n",
    "eval_dataset = TextDataset(\n",
    "    tokenizer=tokenizer,\n",
    "    file_path=\"val_split.txt\",\n",
    "    block_size=128\n",
    ")\n",
    "\n",
    "print(f\"   Blocos de Treino: {len(train_dataset)} | Blocos de Valida√ß√£o: {len(eval_dataset)}\")\n",
    "\n",
    "# O DataCollator agrupa tokens em batches para o modelo,\n",
    "# aplicando m√°scaras ou shifts se necess√°rio.\n",
    "# mlm=False indica que N√ÉO √© Masked Language Modeling (como BERT),\n",
    "# mas um modelo casual (autoregressivo), como o GPT.\n",
    "data_collator = DataCollatorForLanguageModeling(\n",
    "    tokenizer=tokenizer,\n",
    "    mlm=False  # GPT prev√™ o pr√≥ximo token, n√£o preenche m√°scaras\n",
    ")"
   ],
   "metadata": {
    "id": "oKDMwtw7sz9S"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "print(\"Configurando os par√¢metros de treinamento...\")\n",
    "\n",
    "# 'TrainingArguments' define os hiperpar√¢metros e comportamentos do treinamento.\n",
    "training_args = TrainingArguments(\n",
    "    # Onde o modelo e checkpoints ser√£o salvos\n",
    "    output_dir=PASTA_SAIDA,\n",
    "    # Se a pasta j√° existir, sobrescreve sem perguntar\n",
    "    overwrite_output_dir=True,\n",
    "\n",
    "    # N√∫mero total de √©pocas (passadas completas pelo dataset)\n",
    "    num_train_epochs=15,\n",
    "\n",
    "    # Tamanho do batch por GPU/TPU/CPU (aqui, 4 exemplos por passo)\n",
    "    per_device_train_batch_size=4,\n",
    "    per_device_eval_batch_size=4,\n",
    "\n",
    "    # Gradiente √© acumulado por 4 passos antes do update dos pesos\n",
    "    # ‚Äî isso simula um batch maior (4 √ó 4 = 16) sem estourar mem√≥ria)\n",
    "    gradient_accumulation_steps=4,\n",
    "\n",
    "    # Taxa de aprendizado inicial\n",
    "    learning_rate=8e-5,\n",
    "    # N√∫mero de passos antes de atingir a LR m√°xima (esquenta)\n",
    "    warmup_steps=300,\n",
    "\n",
    "    # Esquema de decaimento da LR\n",
    "    lr_scheduler_type=\"linear\",\n",
    "\n",
    "    # Regulariza√ß√£o simples (evita overfitting ‚Äî estamos desativando)\n",
    "    weight_decay=0.0,\n",
    "\n",
    "    # Limita o n√∫mero de checkpoints salvos para economizar espa√ßo\n",
    "    save_total_limit=2,\n",
    "\n",
    "    # Frequ√™ncia e forma de avalia√ß√£o\n",
    "    eval_strategy=\"steps\",     # avaliar ao longo do treino\n",
    "    eval_steps=400,            # avalia a cada 400 passos\n",
    "    save_steps=400,            # salva modelo a cada 400 passos\n",
    "    logging_steps=50,          # loga m√©tricas a cada 50 passos\n",
    "\n",
    "    # Carrega automaticamente o melhor modelo ap√≥s o treino\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"eval_loss\",\n",
    "    greater_is_better=False,   # menor loss √© melhor\n",
    "\n",
    "    # Usa precis√£o mista (float16) ‚Äî acelera em GPUs compat√≠veis\n",
    "    fp16=True,\n",
    "\n",
    "    # Evita o HuggingFace enviar logs para W&B\n",
    "    report_to=\"none\"\n",
    ")\n",
    "\n",
    "# O Trainer √© o orquestrador do processo:\n",
    "# conecta o modelo, datasets, collator, args e executa o treino/valida√ß√£o.\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    data_collator=data_collator,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=eval_dataset,  # Passamos tamb√©m o conjunto de valida√ß√£o\n",
    ")\n",
    "\n",
    "print(\"Configura√ß√£o do Trainer conclu√≠da.\")"
   ],
   "metadata": {
    "id": "oRs5BatzcEQU"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "print(\"Iniciando o fine-tuning... Isso pode levar um bom tempo.\")\n",
    "print(\"Pegue um caf√© ‚òï\")\n",
    "\n",
    "# Inicia o processo de treinamento:\n",
    "# - Faz forward + backward pass\n",
    "# - Atualiza pesos do modelo\n",
    "# - Realiza avalia√ß√µes peri√≥dicas (se configurado)\n",
    "# - Salva checkpoints no diret√≥rio de sa√≠da\n",
    "trainer.train()\n",
    "\n",
    "print(\"üéâ Treinamento conclu√≠do! üéâ\")"
   ],
   "metadata": {
    "id": "l60W-k0BcRGs"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "print(f\"Salvando o modelo final em '{PASTA_SAIDA}'...\")\n",
    "\n",
    "# Salva o modelo j√° fine-tunado no diret√≥rio especificado.\n",
    "# Isso inclui:\n",
    "# - pesos (pytorch_model.bin)\n",
    "# - configura√ß√£o do modelo (config.json)\n",
    "# - estados de treinamento, se existirem (optimizer, scheduler)\n",
    "trainer.save_model()\n",
    "\n",
    "# Salva tamb√©m o tokenizador utilizado no treinamento.\n",
    "# Isso garante que, ao carregar o modelo no futuro,\n",
    "# o texto ser√° tokenizado da mesma forma.\n",
    "tokenizer.save_pretrained(PASTA_SAIDA)\n",
    "\n",
    "print(f\"Modelo e Tokenizador salvos em '{PASTA_SAIDA}'.\")"
   ],
   "metadata": {
    "id": "YSD5xNTQcUnt"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# Instala bibliotecas adicionais necess√°rias para avalia√ß√£o e manipula√ß√£o de texto\n",
    "\n",
    "!pip install bert_score              # Biblioteca para calcular BERTScore ‚Äî m√©trica baseada em embeddings\n",
    "                                     # que avalia similaridade sem√¢ntica entre textos gerados e refer√™ncia\n",
    "\n",
    "!pip install sentence_transformers   # Fornece modelos pr√©-treinados para gerar embeddings de senten√ßas\n",
    "                                     # (ex: SBERT) ‚Äî √∫til para medir similaridade entre frases\n",
    "\n",
    "!pip install nltk                    # Natural Language Toolkit: biblioteca cl√°ssica para NLP\n",
    "                                     # usada para tokeniza√ß√£o, stemming, m√©tricas de texto etc.\n",
    "\n",
    "!pip install pandas                  # Biblioteca para manipula√ß√£o tabular de dados ‚Äî √∫til para an√°lise\n",
    "                                     # e logging de m√©tricas, experimentos, outputs etc.\n",
    "\n",
    "!pip install google-generativeai     # Cliente oficial para acessar modelos generativos da Google (Gemini)\n",
    "                                     # pode ser usado para comparar outputs com outros LLMs"
   ],
   "metadata": {
    "id": "uqfX1MuPw_Zm"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "from bert_score import score as bert_score_calc\n",
    "from nltk.util import ngrams\n",
    "import google.generativeai as genai\n",
    "import random\n",
    "import warnings\n",
    "\n",
    "# Evita spam no console com avisos de bibliotecas\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "\n",
    "# Caminho do modelo fine-tunado a ser auditado\n",
    "CAMINHO_MODELO = \"./modelo_gpt2\"\n",
    "\n",
    "# Corpus de refer√™ncia ‚Äî usado para BERTScore, embeddings e perplexidade\n",
    "ARQUIVO_CORPUS = \"corpus.txt\"\n",
    "\n",
    "# Detecta GPU automaticamente, sen√£o cai no CPU\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# Chave opcional para avaliar textos via Gemini (LLM externo)\n",
    "GEMINI_API_KEY = \"COLE_SUA_CHAVE_API_AQUI\"\n",
    "\n",
    "\n",
    "class AvaliadorClarice:\n",
    "    def __init__(self, modelo_path, corpus_path):\n",
    "\n",
    "        # Carrega tokenizador e modelo GPT-2 fine-tunado\n",
    "        self.tokenizer = GPT2Tokenizer.from_pretrained(modelo_path)\n",
    "        self.model = GPT2LMHeadModel.from_pretrained(modelo_path).to(DEVICE)\n",
    "        self.model.eval()  # Modo de infer√™ncia\n",
    "\n",
    "        # Carrega modelo de embeddings (multi-l√≠ngue, leve e eficiente)\n",
    "        self.embedder = SentenceTransformer('paraphrase-multilingual-MiniLM-L12-v2')\n",
    "\n",
    "        # L√™ corpus original (usado como refer√™ncia liter√°ria)\n",
    "        with open(corpus_path, \"r\", encoding=\"utf-8\") as f:\n",
    "            self.raw_corpus = f.read()\n",
    "\n",
    "        # Divide o corpus em blocos / par√°grafos para compara√ß√£o\n",
    "        self.real_docs = [p for p in self.raw_corpus.split('\\n\\n') if len(p) > 50]\n",
    "\n",
    "\n",
    "    def gerar_amostras(self, n_amostras=20):\n",
    "        \"\"\"Gera textos do modelo para avalia√ß√£o posterior.\"\"\"\n",
    "\n",
    "        # Prompts iniciais estilizados ‚Äî inspirados no estilo da Clarice\n",
    "        prompts = [\"O sil√™ncio\", \"A vida\", \"Eu n√£o\", \"O amor\", \"De repente\", \"A janela\", \"Senti que\"]\n",
    "        textos_gerados = []\n",
    "\n",
    "        for _ in range(n_amostras):\n",
    "            prompt = random.choice(prompts)\n",
    "            inputs = self.tokenizer(prompt, return_tensors=\"pt\").to(DEVICE)\n",
    "\n",
    "            # Gera√ß√£o com sampling (n√£o determin√≠stica)\n",
    "            with torch.no_grad():\n",
    "                out = self.model.generate(\n",
    "                    **inputs,\n",
    "                    max_length=150,\n",
    "                    do_sample=True,\n",
    "                    temperature=1.1,\n",
    "                    repetition_penalty=1.0,\n",
    "                    top_k=40,\n",
    "                    top_p=0.95,\n",
    "                    pad_token_id=self.tokenizer.eos_token_id\n",
    "                )\n",
    "\n",
    "            # Remove tokens especiais e reconverte para texto\n",
    "            texto = self.tokenizer.decode(out[0], skip_special_tokens=True)\n",
    "            textos_gerados.append(texto)\n",
    "\n",
    "        return textos_gerados\n",
    "\n",
    "\n",
    "    def calc_perplexity(self, texto):\n",
    "        \"\"\"Retorna PPL ‚Äî quanto menor, mais o modelo \"entende\" esse texto.\"\"\"\n",
    "\n",
    "        encodings = self.tokenizer(texto, return_tensors=\"pt\")\n",
    "        max_len = self.model.config.n_positions\n",
    "        stride = 512\n",
    "        seq_len = encodings.input_ids.size(1)\n",
    "        nlls = []\n",
    "        prev_end_loc = 0\n",
    "\n",
    "        # Calcula NLL em janelas m√≥veis (evita limite de contexto)\n",
    "        for begin_loc in range(0, seq_len, stride):\n",
    "            end_loc = min(begin_loc + max_len, seq_len)\n",
    "            trg_len = end_loc - prev_end_loc\n",
    "\n",
    "            input_ids = encodings.input_ids[:, begin_loc:end_loc].to(DEVICE)\n",
    "            target_ids = input_ids.clone()\n",
    "            target_ids[:, :-trg_len] = -100  # Ignora tokens fora do trecho\n",
    "\n",
    "            with torch.no_grad():\n",
    "                outputs = self.model(input_ids, labels=target_ids)\n",
    "                nlls.append(outputs.loss)\n",
    "\n",
    "            prev_end_loc = end_loc\n",
    "\n",
    "            if end_loc == seq_len: break\n",
    "\n",
    "        return torch.exp(torch.stack(nlls).mean()).item()\n",
    "\n",
    "\n",
    "    def calc_novelty(self, gerados, n=5):\n",
    "        \"\"\"\n",
    "        Mede o quanto o texto √© 'novo'.\n",
    "        1.0 = totalmente novo\n",
    "        0.0 = c√≥pia literal\n",
    "        \"\"\"\n",
    "        tokens_corpus = self.raw_corpus.split()\n",
    "        corpus_ngrams = set(ngrams(tokens_corpus, n))\n",
    "\n",
    "        scores = []\n",
    "        for texto in gerados:\n",
    "            tokens_gen = texto.split()\n",
    "            if len(tokens_gen) < n: continue\n",
    "\n",
    "            gen_ngrams = list(ngrams(tokens_gen, n))\n",
    "            novos = sum(1 for ng in gen_ngrams if ng not in corpus_ngrams)\n",
    "\n",
    "            ratio = novos / len(gen_ngrams)\n",
    "            scores.append(ratio)\n",
    "\n",
    "        return np.mean(scores)\n",
    "\n",
    "\n",
    "    def calc_embeddings_distance(self, gerados):\n",
    "        \"\"\"\n",
    "        Compara embeddings entre trechos gerados\n",
    "        e trechos reais da Clarice.\n",
    "        Alto = mais parecido\n",
    "        \"\"\"\n",
    "        refs = random.sample(self.real_docs, min(len(gerados), len(self.real_docs)))\n",
    "\n",
    "        emb_real = self.embedder.encode(refs, convert_to_tensor=True)\n",
    "        emb_gen = self.embedder.encode(gerados, convert_to_tensor=True)\n",
    "\n",
    "        cosine_scores = util.cos_sim(emb_gen, emb_real)\n",
    "        return torch.mean(cosine_scores).item()\n",
    "\n",
    "\n",
    "    def calc_bertscore(self, gerados):\n",
    "        \"\"\"\n",
    "        Calcula Precision/Recall/F1 sem√¢ntico usando BERT.\n",
    "        Compara cada texto gerado com uma refer√™ncia aleat√≥ria da Clarice.\n",
    "        \"\"\"\n",
    "        refs = random.sample(self.real_docs, len(gerados))\n",
    "\n",
    "        # Lang=\"pt\" baixa o modelo BERTimbau ou similar automaticamente\n",
    "        P, R, F1 = bert_score_calc(gerados, refs, lang=\"pt\", verbose=False)\n",
    "        return F1.mean().item()\n",
    "\n",
    "\n",
    "    def llm_judge(self, gerados):\n",
    "        \"\"\"\n",
    "        Pede para o Gemini agir como cr√≠tico liter√°rio.\n",
    "        √ötil quando queremos feedback qualitativo.\n",
    "        \"\"\"\n",
    "\n",
    "        if not GEMINI_API_KEY:\n",
    "            return \"N/A\"\n",
    "\n",
    "        genai.configure(api_key=GEMINI_API_KEY)\n",
    "        try:\n",
    "            model_judge = genai.GenerativeModel('gemini-2.5-flash')\n",
    "        except:\n",
    "            return \"Erro Modelo\"\n",
    "\n",
    "        texto_para_avaliar = gerados[0]\n",
    "\n",
    "        prompt = f\"\"\"\n",
    "        Atue como um cr√≠tico liter√°rio especialista em Clarice Lispector.\n",
    "        Avalie o texto abaixo gerado por uma IA.\n",
    "        [...]\n",
    "\n",
    "        Responda APENAS com o n√∫mero da nota.\n",
    "        \"\"\"\n",
    "\n",
    "        try:\n",
    "            response = model_judge.generate_content(prompt)\n",
    "            return response.text.strip()\n",
    "        except Exception as e:\n",
    "            return f\"Erro API: {e}\"\n",
    "\n",
    "\n",
    "    def executar_auditoria(self):\n",
    "\n",
    "        # 1 ‚Äî Avalia perplexidade no corpus real\n",
    "        amostra_validacao = self.raw_corpus[:10000]\n",
    "        ppl = self.calc_perplexity(amostra_validacao)\n",
    "\n",
    "        # 2 ‚Äî Gera textos novos\n",
    "        gerados = self.gerar_amostras(n_amostras=15)\n",
    "\n",
    "        # 3 ‚Äî M√©tricas diversas\n",
    "        bert_f1 = self.calc_bertscore(gerados)\n",
    "\n",
    "        emb_sim = self.calc_embeddings_distance(gerados)\n",
    "\n",
    "        novelty = self.calc_novelty(gerados, n=5)\n",
    "\n",
    "        nota_llm = self.llm_judge(gerados)\n",
    "\n",
    "        # 4 ‚Äî Tabela final\n",
    "        dados = {\n",
    "            \"M√©trica\": [\n",
    "                \"Perplexity\",\n",
    "                \"Novelty\",\n",
    "                \"BERTScore\",\n",
    "                \"Embeddings\",\n",
    "                \"LLM Judge\"\n",
    "            ],\n",
    "            \"Valor\": [\n",
    "                f\"{ppl:.2f}\",\n",
    "                f\"{novelty:.2%}\",\n",
    "                f\"{bert_f1:.4f}\",\n",
    "                f\"{emb_sim:.4f}\",\n",
    "                nota_llm\n",
    "            ],\n",
    "            \"Interpreta√ß√£o Ideal\": [\n",
    "                \"Baixo (< 30)\",\n",
    "                \"Alto (> 90%) mas < 100%\",\n",
    "                \"Alto (> 0.7)\",\n",
    "                \"Alto (> 0.6)\",\n",
    "                \"Alto (> 8.0)\"\n",
    "            ]\n",
    "        }\n",
    "\n",
    "        df = pd.DataFrame(dados)\n",
    "        return df, gerados[0]\n",
    "\n",
    "\n",
    "avaliador = AvaliadorClarice(CAMINHO_MODELO, ARQUIVO_CORPUS)\n",
    "df_resultado, exemplo_texto = avaliador.executar_auditoria()\n",
    "\n",
    "print(f\"Amostra gerada: '{exemplo_texto[:100]}...'\\n\")\n",
    "\n",
    "# No Colab, o display() imprime como tabela HTML\n",
    "display(df_resultado)"
   ],
   "metadata": {
    "id": "YsaLYsEp7Za4"
   },
   "execution_count": null,
   "outputs": []
  }
 ]
}
